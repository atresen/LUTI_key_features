{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import operator\n",
    "from operator import sub\n",
    "from decimal import Decimal\n",
    "from sklearn import preprocessing\n",
    "import statistics\n",
    "import seaborn as sb\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import pybedtools\n",
    "from pybedtools import BedTool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is dedicated to finding the dominant single bp TSS for each gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make bed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file produced by CAGEr with gene names associated\n",
    "\n",
    "df = pd.read_csv('181130single_tss_per_gene.csv')\n",
    "\n",
    "\n",
    "# convert the file to a bed file\n",
    "\n",
    "tlbed = pd.DataFrame(columns=['chr', 'start', 'end', 'name', 'score', 'strand'])\n",
    "\n",
    "tlbed['chr'] = df['coordinates'].str.split(':', expand=True)[0]\n",
    "tlbed['start'] = (df['coordinates']\n",
    "                      .str.split(':', expand=True)[1]\n",
    "                      .str.split('-', expand=True)[0]) \n",
    "tlbed['start'] = tlbed['start'].astype(int)\n",
    "tlbed['end'] = (df['coordinates']\n",
    "                    .str.split(':', expand=True)[1]\n",
    "                    .str.split('-', expand=True)[1])\n",
    "tlbed['name'] = df['genes']\n",
    "tlbed['score'] = 1\n",
    "tlbed['strand'] = df['coordinates'].str.split(':', expand=True)[2]\n",
    "\n",
    "# fill in the end value for single bp peaks and then add one to all\n",
    "tlbed['end'].fillna(value=tlbed['start'], inplace=True)\n",
    "tlbed['end'] = tlbed['end'].astype(int) + 1\n",
    "\n",
    "tlbed.to_csv(\"bedfiles/181130single_tss_per_gene.bed\", sep=\"\\t\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Read in the data for each individual experiment, convert to bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/unallab/tresenrider/programs/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# convert the data for each sample into a bed file\n",
    "\n",
    "# define function\n",
    "\n",
    "def convertBed(x):\n",
    "    x_df = pd.read_csv('TLseq_indiv_samples/'+ x + '_clusters.csv', sep = '\\t', header = None)\n",
    "    y = x_df[[0, 1, 2, 7, 8, 4]]\n",
    "    y[1] = x_df[1] - 1\n",
    "    y = y[y[8] > 1]\n",
    "    y = y[~y[0].str.contains('chr_spike')]\n",
    "    y.to_csv('TLseq_indiv_samples/' + x + '.bed', sep= '\\t', index=False, header=False)\n",
    " \n",
    "\n",
    "# run the function\n",
    "\n",
    "convertBed('TL_2h_R1')\n",
    "convertBed('TL_4h_R1')\n",
    "convertBed('TL_2h_R3')\n",
    "convertBed('TL_4h_R3')\n",
    "convertBed('TL_2h_R5')\n",
    "convertBed('TL_4h_R5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run bedtools intersect with designated file compared to each individual sample's peak calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "\n",
    "def toIntersect(directory, bedA, intersect):\n",
    "    # creates the intersect files\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".bed\"):\n",
    "            print(directory + filename)\n",
    "            a = pybedtools.BedTool(bedA)\n",
    "            b = pybedtools.BedTool(directory + filename)\n",
    "            b.intersect(a, wo = True, s = True).saveas(directory + filename[:-3] + intersect)\n",
    "\n",
    "    # read in the intersect files to python\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(intersect):\n",
    "            print(os.path.join(directory, filename))\n",
    "            x = pd.read_csv(directory + filename, sep = '\\t', header = None)\n",
    "            y = pd.DataFrame(columns = x.columns)\n",
    "            # drop duplicates based on gene name and tpm value\n",
    "            for gene, group in x.groupby(9):\n",
    "                keep_index = group.iloc[:,4].idxmax()\n",
    "                keep = x.loc[keep_index]\n",
    "                y = y.append(keep)\n",
    "            # drop unecessary columns \n",
    "            y.drop([1,2,6,10,11,12], inplace = True, axis = 1)\n",
    "            # add to list of df, these will all be merged later \n",
    "            TSS_df_list.append(y)\n",
    "\n",
    "def toTpm(intersect, bedA):   \n",
    "    TSS_master = TL_2h_R5.merge(TL_2h_R3, how = 'outer', \n",
    "                                    left_on=9, \n",
    "                                    right_on=9, \n",
    "                                    suffixes = ('_2h_R5', '_2h_R3'))\n",
    "    TSS_master = TSS_master.merge(TL_2h_R1, how = 'outer',\n",
    "                                  left_on=9,\n",
    "                                  right_on=9,\n",
    "                                  suffixes = ('_2h_R3', '_2h_R1'))\n",
    "    TSS_master = TSS_master.merge(TL_4h_R5, how = 'outer',\n",
    "                                  left_on=9,\n",
    "                                  right_on=9,\n",
    "                                  suffixes = ('_2h_R1', '_4h_R5'))\n",
    "    TSS_master = TSS_master.merge(TL_4h_R3, how = 'outer',\n",
    "                                  left_on=9,\n",
    "                                  right_on=9,\n",
    "                                  suffixes = ('_4h_R5', '_4h_R3'))\n",
    "    TSS_master = TSS_master.merge(TL_4h_R1, how = 'outer',\n",
    "                                  left_on=9,\n",
    "                                  right_on=9,\n",
    "                                  suffixes = ('_4h_R3', '_4h_R1'))\n",
    "    # reduced columns to only the peak calls for each sample\n",
    "    TSS_select = TSS_master[[9, '3_2h_R5', '3_2h_R3', '3_2h_R1', '3_4h_R5', '3_4h_R3', '3_4h_R1']]\n",
    "    \n",
    "    # set index as gene name\n",
    "    TSS_select.set_index(9, inplace=True)\n",
    "    \n",
    "    # converted strings to floats\n",
    "    TSS_select.astype(float)\n",
    "    \n",
    "    # found the mode for each row and added it to TSS_select\n",
    "    TSS_merge = TSS_select.merge(TSS_select.mode(axis=1), how = 'outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # Merge back to the master which has the tpm values\n",
    "    TSS_master.set_index(9, inplace=True)\n",
    "    TSS_master = TSS_master.merge(TSS_merge, how = 'outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # drop all rows in which there is one clear TSS, these peaks are already figured out\n",
    "    # we just need to do more work with a subset\n",
    "    TSS_2starts = TSS_master.dropna(subset = [1])\n",
    "\n",
    "    # get only the tpm and bp peak columns for each sample\n",
    "    searchfor = ['3_', '4_']\n",
    "    TSS_2starts =  TSS_2starts.loc[:,TSS_2starts.columns.str.contains('|'.join(searchfor))== True]\n",
    "\n",
    "    samples = ['2h_R5', '2h_R3', '2h_R1', '4h_R5', '4h_R3', '4h_R1']\n",
    "\n",
    "    # convert to tuples so that I can associate a peak with the tpm of that cluster? or peak? I don't remember\n",
    "\n",
    "    for i in samples:\n",
    "        TSS_2starts[i] = list(zip(TSS_2starts.loc[:, '3_' + i + '_x'], TSS_2starts.loc[:, '4_' + i]))\n",
    "\n",
    "    # drop all of the non tuple coulmns\n",
    "    TSS_2starts.drop(TSS_2starts.columns.to_series()['3_2h_R5_x':'3_4h_R1_y'], axis=1, inplace=True)\n",
    "    \n",
    "    tuples_ = []\n",
    "\n",
    "    for column, row in TSS_2starts.iterrows():\n",
    "        #print(row)\n",
    "        # find all unique values for the first value of a tuple\n",
    "        my_set = {x[0] for x in row}\n",
    "        # sum all of the second values if their first value is the same \n",
    "        my_sum = [(i,sum(x[1] for x in row if x[0] == i)) for i in my_set]\n",
    "        # find the max value\n",
    "        max_ = max(my_sum, key=lambda x:x[1])\n",
    "        # add to the tuples_ list\n",
    "        tuples_.append(max_)\n",
    "\n",
    "    #add tuples_ as a column called best\n",
    "    TSS_2starts['best']= [i[0] for i in tuples_]\n",
    "    \n",
    "    # add the best column to the TSS_merge df which has all genes\n",
    "    # fill in the best column with previosuly calculated values in column 0\n",
    "\n",
    "    TSS_2starts_best = pd.DataFrame(TSS_2starts['best'])\n",
    "    TSS_merge2 = TSS_merge.merge(TSS_2starts_best, how = 'outer', left_index=True, right_index=True)\n",
    "    TSS_merge2.best.fillna(TSS_merge2.loc[:,0], inplace=True)\n",
    "    \n",
    "    # make a mini df and save as CSV, all it contains are the genes and the most dominant TSSs\n",
    "    TSS_final = TSS_merge2['best']\n",
    "    TSS_final.to_csv('181130TSS_single_bp_' + intersect + '.csv', sep='\\t', header=None)\n",
    "\n",
    "    \n",
    "    # save as a bed! Now it can be used with deeptools2!!!!!\n",
    "    bed = pd.read_csv(bedA, sep='\\t', header=None)\n",
    "    bed_TSS = bed.merge(pd.DataFrame(TSS_final), how = 'outer', left_on=3, right_index=True)\n",
    "    bed_TSS = bed_TSS.dropna('rows')\n",
    "    bed_TSS.iloc[:,1] = (bed_TSS['best'] - 1).astype(int)\n",
    "    bed_TSS.iloc[:,2] = bed_TSS['best'].astype(int)\n",
    "    bed_TSS.iloc[:,4] = bed_TSS.iloc[:,4].astype(int)\n",
    "    bed_TSS.to_csv('bedfiles/181130TSS_single_bp_' + intersect + '.bed', sep='\\t', header=None, index=False)\n",
    "\n",
    "    # this outputs a list with each sample as an entry\n",
    "    # the order in which the files are processed should come out below and the next steps should take this into account\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For all genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLseq_indiv_samples/TL_2h_R1.bed\n",
      "TLseq_indiv_samples/TL_4h_R5.bed\n",
      "TLseq_indiv_samples/TL_4h_R3.bed\n",
      "TLseq_indiv_samples/TL_2h_R3.bed\n",
      "TLseq_indiv_samples/TL_4h_R1.bed\n",
      "TLseq_indiv_samples/TL_2h_R5.bed\n",
      "TLseq_indiv_samples/TL_2h_R5.all\n",
      "TLseq_indiv_samples/TL_4h_R1.all\n",
      "TLseq_indiv_samples/TL_2h_R3.all\n",
      "TLseq_indiv_samples/TL_4h_R3.all\n",
      "TLseq_indiv_samples/TL_4h_R5.all\n",
      "TLseq_indiv_samples/TL_2h_R1.all\n"
     ]
    }
   ],
   "source": [
    "directory = 'TLseq_indiv_samples/'\n",
    "bedA = 'bedfiles/181130single_tss_per_gene.bed'\n",
    "intersect = 'all'\n",
    "\n",
    "# creates a new list\n",
    "TSS_df_list = []\n",
    "\n",
    "toIntersect(directory, bedA, intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/unallab/tresenrider/programs/conda/lib/python3.6/site-packages/ipykernel_launcher.py:110: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# renamed output\n",
    "# note that the order in which the files are in the list is reflected by the above output\n",
    "\n",
    "TL_2h_R5 = TSS_df_list[0]\n",
    "TL_4h_R1 = TSS_df_list[1]\n",
    "TL_2h_R3 = TSS_df_list[2]\n",
    "TL_4h_R3 = TSS_df_list[3]\n",
    "TL_4h_R5 = TSS_df_list[4]\n",
    "TL_2h_R1 = TSS_df_list[5]\n",
    "\n",
    "toTpm(intersect, bedA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For ORF transcripts at LUTI loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLseq_indiv_samples/TL_2h_R1.bed\n",
      "TLseq_indiv_samples/TL_4h_R5.bed\n",
      "TLseq_indiv_samples/TL_4h_R3.bed\n",
      "TLseq_indiv_samples/TL_2h_R3.bed\n",
      "TLseq_indiv_samples/TL_4h_R1.bed\n",
      "TLseq_indiv_samples/TL_2h_R5.bed\n",
      "TLseq_indiv_samples/TL_4h_R1.prox\n",
      "TLseq_indiv_samples/TL_4h_R5.prox\n",
      "TLseq_indiv_samples/TL_2h_R5.prox\n",
      "TLseq_indiv_samples/TL_2h_R1.prox\n",
      "TLseq_indiv_samples/TL_2h_R3.prox\n",
      "TLseq_indiv_samples/TL_4h_R3.prox\n"
     ]
    }
   ],
   "source": [
    "directory = 'TLseq_indiv_samples/'\n",
    "bedA = 'bedfiles/prox_TSS.bed'\n",
    "intersect = 'prox'\n",
    "\n",
    "# creates a new list\n",
    "TSS_df_list = []\n",
    "\n",
    "toIntersect(directory, bedA, intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/unallab/tresenrider/programs/conda/lib/python3.6/site-packages/ipykernel_launcher.py:110: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TL_4h_R1 = TSS_df_list[0]\n",
    "TL_4h_R5 = TSS_df_list[1]\n",
    "TL_2h_R5 = TSS_df_list[2]\n",
    "TL_2h_R1 = TSS_df_list[3]\n",
    "TL_2h_R3 = TSS_df_list[4]\n",
    "TL_4h_R3 = TSS_df_list[5]\n",
    "\n",
    "toTpm(intersect, bedA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For LUTIs :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLseq_indiv_samples/TL_2h_R1.bed\n",
      "TLseq_indiv_samples/TL_4h_R5.bed\n",
      "TLseq_indiv_samples/TL_4h_R3.bed\n",
      "TLseq_indiv_samples/TL_2h_R3.bed\n",
      "TLseq_indiv_samples/TL_4h_R1.bed\n",
      "TLseq_indiv_samples/TL_2h_R5.bed\n",
      "TLseq_indiv_samples/TL_2h_R5.luti\n",
      "TLseq_indiv_samples/TL_2h_R1.luti\n",
      "TLseq_indiv_samples/TL_4h_R1.luti\n",
      "TLseq_indiv_samples/TL_4h_R5.luti\n",
      "TLseq_indiv_samples/TL_4h_R3.luti\n",
      "TLseq_indiv_samples/TL_2h_R3.luti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directory = 'TLseq_indiv_samples/'\n",
    "bedA = 'bedfiles/luti_TSS.bed'\n",
    "intersect = 'luti'\n",
    "\n",
    "# creates a new list\n",
    "TSS_df_list = []\n",
    "\n",
    "toIntersect(directory, bedA, intersect)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/unallab/tresenrider/programs/conda/lib/python3.6/site-packages/ipykernel_launcher.py:110: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TL_2h_5R = TSS_df_list[0]\n",
    "TL_2h_1R = TSS_df_list[1]\n",
    "TL_4h_1R = TSS_df_list[2]\n",
    "TL_4h_5R = TSS_df_list[3]\n",
    "TL_4h_3R = TSS_df_list[4]\n",
    "TL_2h_3R = TSS_df_list[5]\n",
    "\n",
    "toTpm(intersect, bedA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove LUTI-containing loci "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_TSS = pd.read_csv('bedfiles/181130TSS_single_bp_all.bed', sep='\\t', header=None)\n",
    "LUTI_TSS = pd.read_csv('bedfiles/181130TSS_single_bp_luti.bed', sep='\\t', header=None)\n",
    "ORF_TSS = pd.read_csv('bedfiles/181130TSS_single_bp_prox.bed', sep='\\t', header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noluti = all_TSS[~all_TSS.iloc[:,3].isin(LUTI_TSS.iloc[:,3])]\n",
    "noluti_noprox = noluti[~noluti.iloc[:,3].isin(ORF_TSS.iloc[:,3])]\n",
    "\n",
    "noluti_noprox.to_csv('bedfiles/181130TSS_single_bp_nonluti.bed', sep='\\t', header=None, index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
